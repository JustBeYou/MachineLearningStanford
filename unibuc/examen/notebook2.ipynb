{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL as pil\n",
    "import hashlib\n",
    "from pickle import dump, load\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def evaluate_model(model, train_ds, validation_ds, n_jobs = -1, verbose = 1):\n",
    "    cross_scores = cross_val_score(model, train_ds[0], train_ds[1], cv=5, \n",
    "        scoring='accuracy', n_jobs=n_jobs, verbose=verbose)\n",
    "    \n",
    "    model.fit(train_ds[0], train_ds[1])\n",
    "    validation_score = accuracy_score(validation_ds[1], model.predict(validation_ds[0]))\n",
    "    return model, cross_scores, validation_score\n",
    "\n",
    "def create_submission(model, test_index, test_set, filename = 'submission.csv'):\n",
    "    ans = pd.DataFrame({\n",
    "        'id': test_index['image'].apply(lambda img: img.replace('test/', '')), \n",
    "        'label': model.predict(test_set)\n",
    "    })\n",
    "    \n",
    "    ans.to_csv(filename, index=False)\n",
    "\n",
    "def BasicPipeline(model_class):\n",
    "    return Pipeline([\n",
    "        ('norm', PixelNormalizer()),\n",
    "        ('train', model_class())\n",
    "    ])\n",
    "\n",
    "def PixelNormalizer():\n",
    "    return FunctionTransformer(lambda pixel: pixel / 255) \n",
    "\n",
    "def save_dataset(name, obj):\n",
    "    with open(f'{name}.pickle', 'wb') as f:\n",
    "        dump(obj, f)\n",
    "def reload_dataset(name):\n",
    "    with open(f'{name}.pickle', 'rb') as f:\n",
    "        return load(f)\n",
    "\n",
    "def hash_obj(obj):\n",
    "    r = repr(obj).encode('utf-8')\n",
    "    h = hashlib.md5()\n",
    "    h.update(r)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def read_img(filename):\n",
    "    img = np.array(pil.Image.open(filename), dtype=np.uint8)\n",
    "    img = np.reshape(img, (-1))\n",
    "    return img\n",
    "\n",
    "def check_hash(data, h):\n",
    "    return hash_obj(data) == h\n",
    "\n",
    "def check_img_list_hash(imgs, hashes):\n",
    "    return len(imgs) == len(hashes) and all([hash_obj(img) == h for img, h in zip(imgs, hashes)])\n",
    "\n",
    "def load_dataset(dataset_name, load_labels = True):\n",
    "    index = pd.read_csv(f\"{dataset_name}.txt\", header=None, names=['image', 'label', 'hash'])\n",
    "    index['image'] = index['image'].apply(lambda filename: f\"{dataset_name}/{filename}\")\n",
    "    \n",
    "    assert len(index['image']) == 30001 or len(index['image']) == 5000\n",
    "    \n",
    "    if load_labels:\n",
    "        labels = np.array(index[\"label\"], dtype=np.uint8)\n",
    "        labels = np.reshape(labels, (-1))\n",
    "    else:\n",
    "        labels = None\n",
    "    \n",
    "    images = [read_img(filename) for filename in index['image']]\n",
    "    images = np.array(images)\n",
    "    \n",
    "    index['hash'] = [hash_obj(img) for img in images]\n",
    "    \n",
    "    return index, images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reload data\n",
    "train_index, train_set, train_labels = reload_dataset('train')\n",
    "validation_index, validation_set, validation_labels = reload_dataset('validation')\n",
    "test_index, test_set, _ = reload_dataset('test')\n",
    "\n",
    "# Check integrity\n",
    "check_img_list_hash(test_set, test_index['hash'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:   36.2s\n",
      "[Parallel(n_jobs=2)]: Done   3 out of   5 | elapsed:  1.2min remaining:   46.0s\n",
      "[Parallel(n_jobs=2)]: Done   5 out of   5 | elapsed:  1.7min remaining:    0.0s\n",
      "[Parallel(n_jobs=2)]: Done   5 out of   5 | elapsed:  1.7min finished\n"
     ]
    }
   ],
   "source": [
    "basic = BasicPipeline(SGDClassifier)\n",
    "results = evaluate_model(basic, (train_set, train_labels), (validation_set, validation_labels), n_jobs=2, verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Pipeline(steps=[('norm',\n",
       "                  FunctionTransformer(func=<function PixelNormalizer.<locals>.<lambda> at 0x7f1fe28cf4d0>)),\n",
       "                 ('train', SGDClassifier())]),\n",
       " array([0.53491085, 0.53833333, 0.50633333, 0.53416667, 0.5405    ]),\n",
       " 0.5574)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_submission(basic, test_index, test_set, 'sgdc_basic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed: 14.4min remaining: 21.6min\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   5 | elapsed: 14.6min remaining:  9.7min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed: 20.7min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed: 20.7min finished\n"
     ]
    }
   ],
   "source": [
    "svm_basic = BasicPipeline(SVC)\n",
    "results_svm = evaluate_model(svm_basic, (train_set, train_labels), (validation_set, validation_labels), n_jobs=-1, verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Pipeline(steps=[('norm',\n",
       "                  FunctionTransformer(func=<function PixelNormalizer.<locals>.<lambda> at 0x7f200d58c440>)),\n",
       "                 ('train', SVC())]),\n",
       " array([0.72854524, 0.73766667, 0.73383333, 0.72816667, 0.72816667]),\n",
       " 0.735)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_submission(svm_basic, test_index, test_set, 'svm_basic.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
